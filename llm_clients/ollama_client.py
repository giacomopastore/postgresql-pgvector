# llm_clients/ollama_client.py

import requests
from .base_client import LLMClient


class OllamaClient(LLMClient):
    """
    Ollama client class for interacting with Ollama API.
    
    :param api_base: Base URL for the Ollama API.
    :param model: Model name to use in requests.
    """

    def __init__(self, api_base="http://localhost:11434", model="llama2"):
        super().__init__(model)
        self.api_base = api_base

    def generate(self, prompt, max_tokens=50):
        """
        Generates text using the Ollama API.
        
        :param prompt: Text input for the model.
        :param max_tokens: Maximum number of tokens for the response.
        :return: Response generated by the model.
        """
        url = f"{self.api_base}/api/generate"
        data = {
            "model": self.model,
            "prompt": prompt,
            "options": {
                "max_length": max_tokens
            }
        }
        response = requests.post(url, json=data)
        response.raise_for_status()
        return response.json()["response"]

    def embedding(self, text):
        """
        Gets embedding for the given text using the Ollama API and includes detailed response metadata.
        
        :param text: Input text for embedding.
        :return: Dictionary containing embedding and additional response details.
        """
        url = f"{self.api_base}/api/embed"
        data = {
            "model": self.model,
            "input": text
        }
        response = requests.post(url, json=data)
        response.raise_for_status()

        response_data = response.json()
        return {
            "model": response_data.get("model"),
            "embedding": response_data.get("embeddings")[0],
            "total_duration": response_data.get("total_duration"),
            "load_duration": response_data.get("load_duration"),
            "prompt_eval_count": response_data.get("prompt_eval_count")
        }